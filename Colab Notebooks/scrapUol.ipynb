{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"scrapUol.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"HpwE2KyR1MQS","colab":{"base_uri":"https://localhost:8080/","height":367},"executionInfo":{"status":"error","timestamp":1624654338696,"user_tz":180,"elapsed":12,"user":{"displayName":"gianlucca campana","photoUrl":"","userId":"01647816437877561706"}},"outputId":"d6ec3a6b-90b3-422f-b393-e0629a8a8014"},"source":["import re\n","import ssl\n","import mechanize\n","import pandas as pd\n","from requests import get\n","from bs4 import BeautifulSoup\n","from html2text import html2text\n","\n","titles = [] #título da matéria\n","titlesFolha = []\n","links = [] #link da matéria\n","linksFolha = []\n","linksTexto = [] #todos os links encontrados nas páginas\n","linksTextos = []\n","fullPage = [] # todo o texto das páginas\n","fullPages = []\n","\n","def clean_html(html):\n","    cleaned = re.sub(r\"(?is)<(script|style).*?>.*?(</\\1>)\", \"\", html.strip())\n","    cleaned = re.sub(r\"(?s)<!--(.*?)-->[\\n]?\", \"\", cleaned)\n","    cleaned = re.sub(r\"(?s)<.*?>\", \" \", cleaned)\n","    cleaned = re.sub(r\"&nbsp;\", \" \", cleaned)\n","    cleaned = re.sub(r\"  \", \" \", cleaned)\n","    cleaned = re.sub(r\"  \", \" \", cleaned)\n","    return cleaned.strip()\n","\n","def beautifulSoup(url):\n","    ssl._create_default_https_context = ssl._create_unverified_context\n","    response = get(url)\n","    html = BeautifulSoup(response.text, 'html.parser')\n","    return html\n","\n","def getMenuUol(url):\n","    html = beautifulSoup(url)\n","    allNews = html.find_all('li', class_ = 'menu-item')\n","    for news in allNews:\n","        title = news.a.text\n","        titles.append(title)\n","        link = re.sub(r\"^\\//|\\$\", \"https://\", str(news.a['href'].strip()))\n","        links.append(link)\n","\n","def getPegaText(links):\n","    i = 0\n","    if (links[i] != 0):\n","        for link in links:\n","            try:\n","                print('link ' + link + ' sem defeito \\n')\n","                br = mechanize.Browser()\n","                br.set_handle_robots(False)\n","                br.addheaders = [('User-agent', 'Firefox')]\n","                html = br.open(link).read().decode('utf-8')\n","                cleanhtml = clean_html(html)\n","                print(html2text(cleanhtml))\n","                fullPages.append(html2text(cleanhtml))\n","                html_soup = beautifulSoup(link)\n","                for link in html_soup.find_all('a'):\n","                    linksTextos.append(link.get('href'))\n","            except:\n","                print('link ' + str(link) + ' com defeito \\n')\n","\n","def pegaLinkDoLink(links):\n","    for link in links:\n","        html_soup = beautifulSoup(link)\n","        for link in html_soup.find_all('a'):\n","            linksTexto.append(link.get('href'))\n","\n","\n","getMenuUol('https://www.uol.com.br/vivabem/')\n","# test_df = pd.DataFrame({'link': linksTexto})\n","# print(test_df)\n","# print(links)\n","pegaLinkDoLink(links)\n","print(len(linksTexto))\n","getPegaText(linksTexto)\n","test_df = pd.DataFrame({'texto': fullPages})\n","\n","test_df.to_csv('texto.csv')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-217a64589406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmechanize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrequests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mechanize'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"w7vDt-PefJuH"},"source":["import re\n","import ssl\n","import mechanize\n","import pandas as pd\n","from requests import get\n","from bs4 import BeautifulSoup\n","from html2text import html2text\n","\n","titles = [] #título da matéria\n","titlesFolha = []\n","links = [] #link da matéria\n","linksFolha = []\n","linksTexto = [] #todos os links encontrados nas páginas\n","linksTextos = []\n","fullPage = [] # todo o texto das páginas\n","fullPages = []\n","\n","def clean_html(html):\n","    cleaned = re.sub(r\"(?is)<(script|style).*?>.*?(</\\1>)\", \"\", html.strip())\n","    cleaned = re.sub(r\"(?s)<!--(.*?)-->[\\n]?\", \"\", cleaned)\n","    cleaned = re.sub(r\"(?s)<.*?>\", \" \", cleaned)\n","    cleaned = re.sub(r\"&nbsp;\", \" \", cleaned)\n","    cleaned = re.sub(r\"  \", \" \", cleaned)\n","    cleaned = re.sub(r\"  \", \" \", cleaned)\n","    return cleaned.strip()\n","\n","def beautifulSoup(url):\n","    ssl._create_default_https_context = ssl._create_unverified_context\n","    response = get(url)\n","    html = BeautifulSoup(response.text, 'html.parser')\n","    return html\n","\n","def getMenuUol(url):\n","    html = beautifulSoup(url)\n","    allNews = html.find_all('li', class_ = 'menu-item')\n","    for news in allNews:\n","        title = news.a.text\n","        titles.append(title)\n","        link = re.sub(r\"^\\//|\\$\", \"https://\", str(news.a['href'].strip()))\n","        links.append(link)\n","\n","def getPegaText(links):\n","    i = 1\n","    while(i < 50):\n","        try:\n","            link = links[i]\n","            print('link ' + link + ' sem defeito \\n')\n","            br = mechanize.Browser()\n","            br.set_handle_robots(False)\n","            br.addheaders = [('User-agent', 'Firefox')]\n","            html = br.open(link).read().decode('utf-8')\n","            cleanhtml = clean_html(html)\n","            # print(html2text(cleanhtml))\n","            vetores = html2text(cleanhtml).split()\n","            fullPages.append(vetores)\n","            html_soup = beautifulSoup(link)\n","            for link in html_soup.find_all('a'):\n","                linksTextos.append(link.get('href'))\n","            i+=1\n","        except:\n","            print('link ' + str(link) + ' com defeito \\n')\n","            i+=1\n","\n","def pegaLinkDoLink(links):\n","    for link in links:\n","        html_soup = beautifulSoup(link)\n","        for link in html_soup.find_all('a'):\n","            linksTexto.append(link.get('href'))\n","\n","\n","getMenuUol('https://www.uol.com.br/vivabem/')\n","\n","pegaLinkDoLink(links)\n","print(len(linksTexto))\n","getPegaText(linksTexto)\n","for vetores in fullPages:\n","    for vetor in vetores:\n","        test_df = pd.DataFrame({'texto': fullPages})\n","        test_df.to_csv(vetor + 'pi.csv')"],"execution_count":null,"outputs":[]}]}